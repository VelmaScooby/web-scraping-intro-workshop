{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to web scraping using Python\n",
    "## QUT DMRC - 2016\n",
    "\n",
    "Patrik Wikstr√∂m & Brenda Moon\n",
    "\n",
    "### Welcome to the DMRC Web scraping workshop. \n",
    "\n",
    "During this session you will learn how to extract data from all those websites that do not provide structured access to their data via an API. This is a hands-on session where you will be introduced to a set of techniques and tools that will enable you to build a simple web crawler that efficiently collects data from such websites. You will also learn how to store that data in a format suitable for subsequent processing and analysis. Note that this workshop does not require you to have any previous knowledge of programming or web technologies.\n",
    "\n",
    "In this workshop we will collect data from the Metacritic website (http://www.metacritic.com/browse/albums/artist), extracts the fields we are interested in, and then stores the results.\n",
    "\n",
    "* [Exercises (and template for your own web scraping project)](web-scraping-template.ipynb)\n",
    "* [Files](/)\n",
    "\n",
    "We have also created a series of notebooks that build up a script that gets content from the Metacritic website.\n",
    "\n",
    "### Notebooks\n",
    "\n",
    "* Step 1. [Extract album title from a single item on a single page](web-scraping-intro-step1.ipynb)\n",
    "* Step 2. [Extract all album titles on a single page](web-scraping-intro-step2.ipynb)\n",
    "* Step 3. [Extract all review data from a single page](web-scraping-intro-step3.ipynb)\n",
    "* Step 4. [Structure the data extraction as a function](web-scraping-intro-step4.ipynb)\n",
    "* Step 5. [Store the data in a dataframe and save to disk](web-scraping-intro-step5.ipynb)\n",
    "* Step 6. [Restructure the code for clarity](web-scraping-intro-step6.ipynb)\n",
    "* Step 7. [Plotting, tiny stat analysis and improved I/O](web-scraping-intro-step7.ipynb)\n",
    "* Final. [Support for multiple pages](web-scraping-intro-final.ipynb)\n",
    "\n",
    "\n",
    "### Python modules used in this workshop\n",
    "\n",
    "We use three Python modules in this workshop. The full documentation for each is available on their websites:\n",
    "\n",
    "* [Requests](http://docs.python-requests.org/en/latest/) - get webpages from urls\n",
    "* [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/) - select text out of webpages\n",
    "* [Pandas](http://pandas.pydata.org/) - python data analysis library\n",
    "\n",
    "These pages are in [Jupyter Notebook](https://jupyter.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
